M O D U L E - 06

*** 
*** DEMO - 1 - Task ***
*** 

Create and Scale Azure Kubernetes Service cluster 

	az login
	az account set --subscription "SUBS-NAME"
	az group create --name=k8s-aks-cluster-rg-BU --location=eastus
	
	az aks create --resource-group k8s-aks-cluster-rg-BU --name aks-k8s-cluster --disable-rbac --node-count 2 --node-vm-size "Standard_D2_v3" --generate-ssh-keys

	** Note that this command will generate public/private ssh keys in c:\users\super\.ssh folder. 

Install kubectl tool which allows you to run commands against Kubernetes cluster

	Windows:
     choco install -y kubernetes-cli

	Linux: 
	  sudo apt-get -y update && sudo apt-get -y upgrade  
	  curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl
	  chmod +x ./kubectl
	  sudo mv ./kubectl /usr/local/bin/kubectl

In Linux, due to current bug with azure-cli you need to revert to 2.0.23-1 version. 

	sudo apt-get -y purge azure-cli
	sudo apt-get -y install azure-cli=2.0.23-1

	az aks get-credentials --resource-group k8s-aks-cluster-rg-BU --name aks-k8s-cluster

	** the local config file C:\users\super\.kube\config	

	kubectl config set-context aks-k8s-cluster
	kubectl create deployment nginx --image=nginx
	kubectl expose deployment nginx --port=80 --type=LoadBalancer
	kubectl get service nginx
	curl http://$( kubectl get service nginx -o=jsonpath='{.status.loadBalancer.ingress[*].ip}')
	kubectl get pods
	kubectl get replicaset
	kubectl scale --replicas=3 deployment/nginx
	kubectl get pods -o wide
	kubectl get nodes
	
	** Just show below command, it takes time to be completed!!
	az aks scale --node-count=3 --resource-group k8s-aks-cluster-rg-BU --name aks-k8s-cluster

	** You can also look at the maximum pod capacity of a node by running the command:
	
	kubectl get node NODE-NAME -o=jsonpath='{.status.capacity.pods}'

	** Just show below command, it takes time to be completed!!
	az aks scale --node-count=2 --resource-group k8s-aks-cluster-rg-BU --name aks-k8s-cluster

	kubectl delete deployment nginx
	kubectl delete service nginx

*** 
*** DEMO - 2 - Task ***
*** 

Install Minikube – Complete this task and minikube install before your demo
	
	1.	Sign into your LOD Ubuntu VM. 
	2.	Open a terminal by right clicking anywhere on the Desktop. 
	3.	Go into root and type in your password when prompted.  The Password for the VM is: P@ssw0rd123!

		sudo -i
 
	4.	Run the following commands:
 	
		rm /var/lib/apt/lists/lock
 		rm /var/cache/apt/archives/lock 
		rm /var/lib/dpkg/lock

	5.	Navigate to the lab files by running:

		cd labs/module5

	6.	You are now to install Minikube. 
		Run the following two commands (the first command gives execute permission to your script, 
		the second script runs it):

		chmod +x ./install-minikube.sh
		./install-minikube.sh

Deploy Nginx

	1.	Create a deployment and name it “nginx”
	
		kubectl run nginx --image=nginx --port=80 

	2.	Expose the deployment using a service 
	
		kubectl expose deployment nginx --type=NodePort

	3.	Access the nginx default web page using the curl command.

		curl $(minikube service nginx --url) 

Working with the health probe  

	In this task you will create a new pod and enable a health probe. 
	To test the probe, pod will run a single container that is going to 
	explicitly fail the health probe request after every 5 probes. 
	
	1.	You should still be in the /labs/module5 folder, if not, navigate to there. 
	
	2.	Create the pod using the yaml file:

		kubectl apply -f  liveness-probe.yaml

	3.	Check the status of the newly created pod. 
		It may take few seconds for the container to be up and running.

		kubectl get pods

		Notice the STATUS column shows Running and RESTARTS column have the value zero. 
		That’s expected because container is just started, and the health probe has not failed yet.

	4.	After 3-4 minutes if you view the status of pods again you should see 
		the RESTARTS column with the value 1 (or higher depending on how long you have 
		waited to check the status of the pod)

	5.	Behind the scenes, every time a container fails the health probe it will be restarted again. 
		To get bit more information about the failing health probe run the following command:
	
		kubectl describe po liveness-probe

	6.	Eventually after failing the health probes multiple times in a short interval 
		container will be put under "CrashLoopBackOff" status.  

	7.	You can view the logs from the container that is terminated by using the command:
	
		kubectl logs liveness-probe --previous

	8.	Finally, remove the pod 

		kubectl delete pod liveness-probe

	








Working with Replica Set

	1.	In this task you will first create a replica with predefined labels assigned to pods. 
		Later you will change the labels for a pod and observer behavior of replica set. 

		The nginx-rc.yaml file is available inside the 
		/labs/module5 
		subfolder and contains definition of replica set. 
		If you review the content of file you will notice that it will maintain 
		3 pods with each running nginx container. Pods are also labeled app=webapp.

		To create the replica set and pods run the following command in the labs/module5 directory.

		kubectl create -f  nginx-rc.yaml

	2.	Let’s look at the pods along with their labels. 

		kubectl get pods --show-labels
 
		** You can also list all the replica sets that are available by using the command:

		kubectl get replicaset
 
	3.	Notice we have three pods running. 
		If you delete one of them, replica set will ensure that total pods count remain three 
		and it will do that by creating a new pod. 
		
		** First delete one of the pods (get name from this command: kubectl get pods --show-labels)

		kubectl delete pod <<pod-name>>  

		** Now, check the pods again. 
		Notice you still have three pods running and one of them is terminating.  

		kubectl get pods --show-labels
 
	4.	Another factor that plays an important role in determining pods relationship 
		with replica set is the labels. 
		Currently app=webapp is the selector used by replica set to determine the pods under its watch. 
		If you change the label of a pod from app=webapp to say app=debugging 
		then replica set will effectively remove it from its watch and create another pod 
		with the label app=webapp. 
		For replica set its job is to maintain the total count of pods to three as per the definition 
		provided in the yaml file. 

		kubectl label pod <<pod-name>> app=debugging --overwrite=true

	5.	View the pods again and notice that there are four pods running. 
		Replica set created an additional pod immediately after it noticed pod count was less than three. 

		kubectl get pods --show-labels
 
	6.	Replica set is essentially using selector (defined in the yaml file) 
		to which pods to observe. 
		In this case its label app matching value webapp. 
		You can also get all the pods with app=webapp label using the following command. 

		kubectl get pods --show-labels -l app=webapp

	7.	Finally remove the replica set using the following command. 

		kubectl delete replicaset nginx-replica

	8.	As part of the deletion process replica set will remove all the pods that it had created.  
		You can see that by listing the pods and looking at the STATUS column which shows Terminating. 

		kubectl get pods
 
		Eventually pods will be removed. 
		However, if you list the pods again the pod with label app=debugging is still Running. 

		kubectl get pods --show-labels
 
		Since you have change the label this pod is no longer manage by the replica set. 
		In cases like these you can bulk remove pods based on labels.  

		kubectl delete pods -l app=debugging

Working with Deployments

	1.	In this task you will begin by performing a deployment based on 
		specific version of nginx container image (v 1.7.9). 
		Later you will leverage RollingUpdate strategy for deployment to update 
		pods running nginx container image from v1.7.9 to container image 1.8.

		The nginx-deployment.yaml file is available inside the 
		/labs/module5 
		subfolder and contains definition of deployment. 
		If you review the content of file you will notice that it will maintain 
		2 pods with each running nginx container image v1.7.9. 
		Pods are also labeled app=nginx. 

		** Run the following command from the labs/module5 directory:

		kubectl create -f nginx-deployment.yaml

	2.	Notice the deployment status by running the command:
		
		kubectl get deployment
	 
	3.	If you list the pods you should see the out similar to following:

		kubectl get pods --show-labels
 
		Notice the LABELS column and presence of pod-template-hash label. 
		This label is used by the deployment during the update process.

	4.	You are now going to update the deployment. 
		You are going to update nginx container image from v 1.7.9 to v1.8. 
		Before you do that first check the existing definition of the deployment:

		kubectl describe deployment nginx-deployment
 
		Notice the line Image:    nginx:1.7.9 
		which confirms that the current deployment is using 1.7.9 version of nginx image. 

	5.	Perform the update using the command below.  

		kubectl apply -f nginx-deployment-updated.yaml

		If you review the content of nginx-deployment-updated.yaml file and 
		compare it with original  nginx-deployment-updated.yaml 
		the only difference is the image tag which is changed from  1.7.9 to 1.8.

	6.	If you immediately (after step 5) run the command to list all the pods 
		you should see output like following:

		kubectl get pods --show-labels
 
		Notice that the deployment strategy of rolling update 
		ensures that the old pods (nginx v 1.7.9) are terminated 
		only after new pods (nginx image v 1.8) are in a running state. 

		Also notice that the label pod-template-hash values are different for old and new pods. 
		This is because the pod definition (due to change of image tag) is not same for both deployments.

	7.	You can also look at the new deployment details and 
		make sure that correct nginx image (v 1.8) is used.

		kubectl describe deployment nginx-deployment
